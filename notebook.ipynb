{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c14de227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "\n",
    "# find M closest\n",
    "\n",
    "def closest(x0,x1,sigma=1.0,M=5):\n",
    "    # which ensembles x0 are closzest to x1\n",
    "    # measure distance\n",
    "    d=np.abs(x0-x1)\n",
    "    \n",
    "    # just see how many are in spec\n",
    "    w = np.where(d<3*sigma)\n",
    "    if (len(w[0])<M):\n",
    "        return None\n",
    "    print(f'{len(w[0])} samples within spec.')\n",
    "    \n",
    "    # sort the best M\n",
    "    indices=np.argsort(d[:M])\n",
    "    # M lowest distance\n",
    "    return d[indices]\n",
    "\n",
    "def gaussian(x,mu,sigma):\n",
    "    z=(x-mu)/(2.*sigma)\n",
    "    g = np.exp(-z*z)\n",
    "    return g/g.sum()\n",
    "\n",
    "def varsolution(x0,x1,s0=1.0,s1=1.0):\n",
    "    num = (x0/(s0*s0)) + (x1/(s1*s1))\n",
    "    den = (1.0/(s0*s0)) + (1.0/(s1*s1))\n",
    "    return num/den,np.sqrt(1./den)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1892d",
   "metadata": {},
   "source": [
    "# Issues with matching N samples from a distribution as a DA constraint\n",
    "\n",
    "## Example 1\n",
    "\n",
    "We have a 1D Gaussian prior $B$ with mean $x_b=0$ and standard deviation $σ_b=1.\n",
    "\n",
    "We take an observation of $x$, $x_{obs}=5$ with measurement uncertainty $σ_{obs}=1$. \n",
    "\n",
    "We want to use DA to estimate the posterior distribution, based on the prior constraint and the observation. We have a *truth* here, in that the variational approach for Gaussian distributions gives us a simple analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "637977f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xobs = 5.\n",
    "xb = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fbf1bf",
   "metadata": {},
   "source": [
    "\n",
    "What is the posterior mean?\n",
    "\n",
    "This is trivial from Gaussian stats and variational analysis, and you end up with:\n",
    "\n",
    "$$\n",
    "x=\\frac{\\frac{x_{obs}}{σ_{obs}^2} + \\frac{x_b}{σ_b^2}}{\\frac{1}{σ_{obs}^2} + \\frac{1}{σ_b^2}}\n",
    "$$\n",
    "\n",
    "Which is just a reciprocal-uncertainty weighted average. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1ad9d96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 2.50 sd 0.707\n"
     ]
    }
   ],
   "source": [
    "mean,std = varsolution(xobs,xb)\n",
    "print(f'mean {mean:.2f} sd {std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409bc6b",
   "metadata": {},
   "source": [
    "This makes sense, intuitively as well: the inputs have the same uncertainty, so the posterior will be half-way between prior and observation. The output uncertainty from a mean should be better than from any one sample (1/√N). This is the (variational) Data Assimilation result. This is what you have said you are solving. There is no judgement here on whether its good or bad. This is the solution according to the assumptions made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87867f",
   "metadata": {},
   "source": [
    "# Whjat are you doing in what you claim is variational DA\n",
    "\n",
    "Now take your approach to this problem:\n",
    "\n",
    "\tYou randomly sample the prior and take the closest M matches\n",
    "\tYou generate posterior statistics from those matches\n",
    "\n",
    "You generate e.g. 1e5 samples over $B$. You think you need that many because otherwise we never sample at x=100, because from the prior its pretty unlikely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3891476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336 samples within spec.\n",
      "mean 4.84 sd 1.212\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "xb_s = normal(xb,size=100000)\n",
    "xobs_s = xobs\n",
    "\n",
    "samples = closest(xb_s,xobs_s,M=5)\n",
    "mean,std = np.mean(samples),np.std(samples)\n",
    "print(f'mean {mean:.2f} sd {std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8bbb3a",
   "metadata": {},
   "source": [
    "This is clearly crazy as a result. Its nowhere near the analytical solution we calculate above. If you run the cell again, you'll also see that the estimate of the mean and standard deviation is pretty unstable.\n",
    "\n",
    "Hacking around\n",
    "=============\n",
    "\n",
    "You think maybe you haven't used enough samples. Or maybe $M$ is too low. Let's change $M$ as we have 2200-odd samples within spec, let's put it at 2000 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2a462048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2255 samples within spec.\n",
      "[1.57554051 1.8184968  1.86999191 ... 8.32948919 8.47914554 9.20409533]\n",
      "mean 5.01 sd 1.011\n"
     ]
    }
   ],
   "source": [
    "# your solution M=2000\n",
    "xb_s = normal(xb,size=100000)\n",
    "xobs_s = xobs\n",
    "\n",
    "samples = closest(xb_s,xobs_s,M=2000)\n",
    "print(samples)\n",
    "mean,std = np.mean(samples),np.std(samples)\n",
    "print(f'mean {mean:.2f} sd {std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2e99c",
   "metadata": {},
   "source": [
    "Hmmm... thats not right either. Maybe its sample number: let's put that high now, and increase M at the same time. Lets test stability whilst we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c8b35597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227581 samples within spec.\n",
      "mean 5.00 sd 1.002\n",
      "227999 samples within spec.\n",
      "mean 5.00 sd 1.001\n",
      "227732 samples within spec.\n",
      "mean 5.00 sd 0.999\n",
      "227839 samples within spec.\n",
      "mean 5.00 sd 0.999\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    xb_s = normal(xb,size=10000000)\n",
    "    samples = closest(xb_s,xobs_s,M=200000)\n",
    "    mean,std = np.mean(samples),np.std(samples)\n",
    "    print(f'mean {mean:.2f} sd {std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d373c09",
   "metadata": {},
   "source": [
    "Maybe I shouldn't have put M up ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fedc5093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228005 samples within spec.\n",
      "mean 5.83 sd 0.475\n",
      "227528 samples within spec.\n",
      "mean 4.85 sd 1.011\n",
      "227769 samples within spec.\n",
      "mean 5.24 sd 0.427\n",
      "227762 samples within spec.\n",
      "mean 4.86 sd 1.075\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    xb_s = normal(xb,size=10000000)\n",
    "    samples = closes\n",
    "    \n",
    "    \n",
    "    \n",
    "    t(xb_s,xobs_s,M=5)\n",
    "    mean,std = np.mean(samples),np.std(samples)\n",
    "    print(f'mean {mean:.2f} sd {std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4001893",
   "metadata": {},
   "source": [
    "Nope. \"OK, well, at least the one with high M is quite stable now, and gives a solution thats close to my observation, so it must be ok, right? Wasn't that what I really wanted in the first place, rather than the variational compromise I said I was looking for? The posterior matches my measurement of state, so this is perfect then.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df43c7",
   "metadata": {},
   "source": [
    "**NO** That is just hacking around the mathematics, and making post-hoc decisions about what you wanted. It is not rigorous. It is not a solution to the problem that was laid out.\n",
    "\n",
    "In fact, it is, in essence, **a solution based only on the observation**, and has no real sensitivity to the prior. If that's what you wanted, then you should have said so at the start. That might be appropriate in some cases. \n",
    "\n",
    "But its **not** data assimilation, is it? and the solution is not (much) constrained by the prior. If you had wanted to assume a weaker prior, thats what you should have done.\n",
    "\n",
    "And also, it will only be appropriate when there is enough information in the observations to properly constrain the estimates of state. That is *not* the case in remote sensing, where we believe we have an ill-posed problem. An ill-posed problem is one **that is very sensitive to noise**, so the various efforts you have done in this, fitting to a single observation, make even less sense to me.\n",
    "\n",
    "\"But isn't that just what other people do in ensemble methods\" you say?\n",
    "\n",
    "**NO** Not if they are trying to solve the same problem we have laid out, unless they are just hacking around themselves, and shouldn't be publishing such things.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
